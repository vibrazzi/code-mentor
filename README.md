# CodeMentor ¬∑ IA para L√≥gica de Programa√ß√£o

CodeMentor √© um assistente especializado em l√≥gica de programa√ß√£o, constru√≠do com FastAPI e alimentado por **Groq API** (llama-3.1-8b-instant) ou Ollama local. O projeto foi pensado para portf√≥lios profissionais: interface moderna, backend resiliente, respostas ultra-r√°pidas (~300ms) e pipeline de deploy via Railway.

---

## Sum√°rio

1. [Vis√£o Geral](#vis√£o-geral)
2. [Principais Recursos](#principais-recursos)
3. [Arquitetura](#arquitetura)
4. [Pr√©-requisitos](#pr√©-requisitos)
5. [Execu√ß√£o Local](#execu√ß√£o-local)
6. [Deploy no Railway](#deploy-no-railway)
7. [Configura√ß√£o por Ambiente](#configura√ß√£o-por-ambiente)
8. [Testes e Qualidade](#testes-e-qualidade)
9. [Personaliza√ß√£o](#personaliza√ß√£o)
10. [Pr√≥ximos Passos](#pr√≥ximos-passos)
11. [Licen√ßa](#licen√ßa)

---

## Vis√£o Geral

- **Objetivo:** oferecer mentoria de l√≥gica de programa√ß√£o em portugu√™s, com explica√ß√µes graduais, exemplos em Python e analogias acess√≠veis.
- **Modelo base:** `llama-3.1-8b-instant` via **Groq API** (padr√£o) ou `llama3.2:3b` via Ollama local.
- **Stack principal:** FastAPI ¬∑ Groq API ¬∑ Jinja2 ¬∑ Vanilla JS ¬∑ CSS customizado.
- **Performance:** Respostas em **~300-500ms** com Groq (vs 10-30s com Ollama local em CPU).

---

## Principais Recursos

- **‚ö° Ultra-r√°pido:** Groq API com hardware dedicado entrega respostas em milissegundos
- **Respostas estruturadas:** reconhecimento do problema, teoria resumida, exemplo em Python, convite √† pr√°tica
- **Conversas com contexto:** hist√≥rico recente para manter coer√™ncia
- **Interface responsiva:** layout futurista, sugest√µes r√°pidas, indicador de digita√ß√£o e acessibilidade (ARIA)
- **Dual provider:** suporta Groq API (recomendado) ou Ollama local como fallback
- **Resili√™ncia:** retry logic, tratamento de erros e valida√ß√£o via Pydantic
- **Pronto para produ√ß√£o:** Deploy no Railway em minutos

---

## Arquitetura

```
code-mentor/
‚îú‚îÄ‚îÄ main.py                # Aplica√ß√£o FastAPI e integra√ß√£o com Ollama
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ css/styles.css     # Tema futurista responsivo
‚îÇ   ‚îî‚îÄ‚îÄ js/app.js          # UX do chat (hist√≥rico, feedback visual, fetch API)
‚îú‚îÄ‚îÄ templates/index.html   # Template Jinja2 da interface
‚îú‚îÄ‚îÄ chat_terminal.py       # CLI para conversas r√°pidas (debug)
‚îú‚îÄ‚îÄ tests/test_app.py      # Testes unit√°rios
‚îú‚îÄ‚îÄ Dockerfile             # Build completo com Ollama + aplica√ß√£o
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python
‚îú‚îÄ‚îÄ railway.json           # Comando de start para deploy no Railway
‚îî‚îÄ‚îÄ README.md
```

> **Nota:** mantenha o Ollama rodando em um servi√ßo dedicado no ambiente de produ√ß√£o. Executar modelo e API no mesmo container exige recursos elevados.

---

## Pr√©-requisitos

- Python **3.12** ou superior
- [Ollama](https://ollama.com) instalado e operando (`ollama serve`)
- Modelo baixado localmente (`ollama pull llama3.2:3b`) ou endpoint remoto compat√≠vel
- Docker (opcional) para build e deploy containerizado

---

## Execu√ß√£o Local

```bash
git clone https://github.com/seu-usuario/code-mentor.git
cd code-mentor

python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

pip install -r requirements.txt
ollama pull llama3.2:3b     # execute apenas na primeira vez

uvicorn main:app --reload
```

Abra `http://127.0.0.1:8000` para iniciar a sess√£o com o mentor.

---

## Execu√ß√£o com Docker

```bash
docker build -t codementor .
docker run --rm \
  -e OLLAMA_URL=http://host.docker.internal:11434/api/generate \
  -e OLLAMA_MODEL=llama3.2:3b \
  -p 8000:8000 codementor
```

> O container n√£o inclui o Ollama. Execute `ollama serve` e `ollama pull <modelo>` no host e exponha a porta 11434. Em macOS/Windows use `host.docker.internal`; em Linux utilize o IP da m√°quina anfitri√£ ou uma rede Docker customizada.

---

## Deploy no Railway

### ‚ö° Op√ß√£o 1: Com Groq API (Recomendado - Ultra R√°pido)

1. **Obtenha uma API Key da Groq:**

   - Acesse: https://console.groq.com/
   - Fa√ßa login e crie uma API Key gratuita
   - Copie a chave (come√ßa com `gsk_...`)

2. **Deploy no Railway:**

   - Empurre o reposit√≥rio para o GitHub
   - No Railway, crie um projeto usando **Deploy from GitHub repo**
   - Adicione as vari√°veis de ambiente:
     ```
     LLM_PROVIDER=groq
     LLM_API_KEY=gsk_sua_chave_aqui
     LLM_MODEL=llama-3.1-8b-instant
     LLM_API_URL=https://api.groq.com/openai/v1/chat/completions
     LLM_MAX_TOKENS=800
     ```

3. **Pronto!** ‚ú®
   - Deploy autom√°tico em 1-2 minutos
   - Respostas em ~300-500ms
   - Plano gratuito: 14,400 requests/dia
   - Custo zero de infraestrutura

### üîß Op√ß√£o 2: Com Ollama Local (Fallback)

1. Crie dois servi√ßos no Railway:

   - **Servi√ßo 1:** FastAPI (este reposit√≥rio)
   - **Servi√ßo 2:** Ollama dedicado (usando `deploy/ollama/Dockerfile`)

2. Configure as vari√°veis no servi√ßo FastAPI:

   ```
   LLM_PROVIDER=ollama
   OLLAMA_URL=http://<nome-do-servico-ollama>:11434/api/generate
   OLLAMA_MODEL=llama3.2:3b
   ```

3. O Ollama usa o script `deploy/ollama/entrypoint.sh` que:
   - Baixa o modelo automaticamente na primeira inicializa√ß√£o
   - Usa volume persistente para armazenar o modelo
   - Inicia o `ollama serve` automaticamente

> **Nota:** Ollama em CPU no Railway √© ~60-100x mais lento que Groq (~15-30s vs 0.3-0.5s)

---

## Configura√ß√£o por Ambiente

### Vari√°veis Groq API (Recomendado)

| Vari√°vel          | Padr√£o                                            | Descri√ß√£o                                                                              |
| ----------------- | ------------------------------------------------- | -------------------------------------------------------------------------------------- |
| `LLM_PROVIDER`    | `ollama`                                          | Provider: `groq` (recomendado) ou `ollama`                                             |
| `LLM_API_KEY`     | -                                                 | **Obrigat√≥rio para Groq** - API Key da Groq Console                                    |
| `LLM_API_URL`     | `https://api.groq.com/openai/v1/chat/completions` | Endpoint da Groq API                                                                   |
| `LLM_MODEL`       | `llama-3.1-8b-instant`                            | Modelo Groq: `llama-3.1-8b-instant`, `llama-3.1-70b-versatile` ou `mixtral-8x7b-32768` |
| `LLM_MAX_TOKENS`  | `800`                                             | Limite de tokens por resposta (50-2000)                                                |
| `LLM_TEMPERATURE` | `0.7`                                             | Criatividade: 0.0 (conservador) - 2.0 (criativo)                                       |
| `LLM_TOP_P`       | `0.9`                                             | Nucleus sampling (0.0-1.0)                                                             |

### Vari√°veis Ollama Local (Fallback)

| Vari√°vel             | Padr√£o                                | Descri√ß√£o                               |
| -------------------- | ------------------------------------- | --------------------------------------- |
| `OLLAMA_URL`         | `http://localhost:11434/api/generate` | Endpoint da API do Ollama               |
| `OLLAMA_MODEL`       | `llama3.2:3b`                         | Identificador do modelo                 |
| `OLLAMA_NUM_PREDICT` | `150`                                 | Limite de tokens gerados                |
| `OLLAMA_NUM_CTX`     | `512`                                 | Tamanho do contexto                     |
| `OLLAMA_NUM_THREAD`  | `8`                                   | Threads de infer√™ncia (ajuste para CPU) |
| `OLLAMA_NUM_BATCH`   | `512`                                 | Batch size interno                      |
| `OLLAMA_TEMPERATURE` | `0.7`                                 | Controle de criatividade                |
| `OLLAMA_TOP_P`       | `0.9`                                 | Nucleus sampling                        |

### Vari√°veis Gerais

| Vari√°vel                  | Padr√£o | Descri√ß√£o                          |
| ------------------------- | ------ | ---------------------------------- |
| `REQUEST_TIMEOUT_SECONDS` | `60`   | Timeout das requisi√ß√µes (segundos) |

**Exemplo Groq:**

```bash
export LLM_PROVIDER="groq"
export LLM_API_KEY="gsk_..."
export LLM_MODEL="llama-3.1-8b-instant"
uvicorn main:app
```

**Exemplo Ollama:**

```bash
export LLM_PROVIDER="ollama"
export OLLAMA_URL="http://localhost:11434/api/generate"
uvicorn main:app
```

---

## Testes e Qualidade

```bash
source .venv/bin/activate
python -m unittest discover -s tests
```

Os testes atuais validam:

- Renderiza√ß√£o da homepage
- Fluxo feliz do endpoint `/api/chat` (mockando o Ollama)
- Regras de valida√ß√£o (payload vazio ‚Üí HTTP 422)

Inclua testes adicionais conforme evoluir o projeto (por exemplo, testes end-to-end com Playwright).

---

## Personaliza√ß√£o

- **Prompt e tom:** ajuste a constante `PROMPT_SISTEMA` em `main.py`.
- **Layout e identidade visual:** edite `static/css/styles.css` ou substitua o template `templates/index.html`.
- **Comportamento do chat:** altere `static/js/app.js` para mudar regras de hist√≥rico, anima√ß√µes e UX.
- **Integra√ß√£o com outros modelos:** atualize `OLLAMA_MODEL` ou adapte `_call_ollama` para consumir APIs externas.

---

## Pr√≥ximos Passos

- [ ] Implementar autentica√ß√£o para acompanhar hist√≥rico individual.
- [ ] Adicionar m√©tricas de uso e logs estruturados.
- [ ] Criar testes end-to-end para a camada de frontend.
- [ ] Disponibilizar integra√ß√µes com outros idiomas.

---

## Licen√ßa

Projeto de portf√≥lio. Utilize como base educacional ou ponto de partida profissional. Ao compartilhar melhorias, cite a autoria original.
